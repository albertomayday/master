# --- Ensure professional import style: always allow 'from scripts...' ---
import sys
import pathlib
project_root = pathlib.Path(__file__).parent.parent.resolve()
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))
#!/usr/bin/env python3
"""
Test Documentation System

Script simple para probar el sistema de documentaciÃ³n sin dependencias externas.
Verifica que todos los componentes funcionen correctamente en modo fallback.

Autor: Sistema de DocumentaciÃ³n
Fecha: 2024
"""

import os
import sys
from pathlib import Path

# AÃ±adir el directorio del proyecto al path
repo_root = Path(__file__).parent.parent
sys.path.insert(0, str(repo_root))


def test_auto_updater():
    """Probar el sistema de auto-actualizaciÃ³n"""

    print("\nğŸ”„ Testing Auto-Update System...")

    try:
        # Import local
        from scripts.auto_update_docs import DocumentationAutoUpdater

        # Crear instancia
        updater = DocumentationAutoUpdater()

        # Test bÃ¡sico - verificar que inicializa correctamente
        print(f"âœ… Auto-updater initialized")
        print(f"   ğŸ“ Repo path: {updater.repo_path}")
        print(f"   ğŸ“„ Docs path: {updater.docs_path}")

        # Test de detecciÃ³n de cambios simulados
        import asyncio
        from datetime import datetime, timedelta

        # Ejecutar detecciÃ³n de cambios
        last_scan = datetime.now() - timedelta(hours=6)
        changes = asyncio.run(updater.detect_code_changes(since=last_scan))

        print(f"   ğŸ“Š Changes detected: {len(changes)}")

        if changes:
            for change in changes[:3]:  # Mostrar primeros 3
                print(
                    f"     â€¢ {change.file_path}: {len(change.function_changes)} functions, {len(change.class_changes)} classes"
                )

        print("âœ… Auto-updater system working correctly")
        return True

    except Exception as e:
        print(f"âŒ Auto-updater test failed: {e}")
        return False


def test_dashboard_cli():
    """Probar el dashboard en modo CLI"""

    print("\nğŸ“Š Testing Dashboard System...")

    try:
        # Import local
        from scripts.documentation_dashboard import CLIDashboard, DocumentationDashboard

        # Probar dashboard bÃ¡sico
        dashboard = DocumentationDashboard()

        print(f"âœ… Dashboard initialized")
        print(f"   ğŸ“ Repo path: {dashboard.repo_path}")

        # Test de obtenciÃ³n de documentos
        docs = dashboard.get_all_documentation_files()
        print(f"   ğŸ“„ Documentation files found: {len(docs)}")

        if docs:
            for doc in docs[:3]:  # Mostrar primeros 3
                print(f"     â€¢ {doc.name}: {doc.word_count} words, {doc.code_blocks} code blocks")

        # Test de bÃºsqueda
        if docs:
            results = dashboard.search_documentation("ML integration")
            print(f"   ğŸ” Search results for 'ML integration': {len(results)}")

        # Test health metrics
        health = dashboard.calculate_health_metrics()
        print(f"   ğŸ¥ System health score: {health['overall_score']:.1f}/100")

        print("âœ… Dashboard system working correctly")
        return True

    except Exception as e:
        print(f"âŒ Dashboard test failed: {e}")
        return False


def test_simulation_data():
    """Probar el generador de datos simulados"""

    print("\nğŸ­ Testing Simulation Data Generator...")

    try:
        # Import local
        from scripts.generate_simulation_data import DocumentationDataSimulator

        # Crear instancia
        simulator = DocumentationDataSimulator()

        print(f"âœ… Simulator initialized")
        print(f"   ğŸ“ Data path: {simulator.data_dir}")

        # Generar datos de prueba
        analytics = simulator.generate_usage_analytics(7)  # 7 dÃ­as

        print(f"   ğŸ“Š Analytics generated:")
        print(f"     â€¢ Total views: {analytics['overview']['total_views']}")
        print(f"     â€¢ Documents tracked: {len(analytics['popular_documents'])}")
        print(f"     â€¢ Daily trends: {len(analytics['trends'])} days")

        # Health metrics
        health = simulator.generate_system_health()
        print(f"   ğŸ¥ Health metrics:")
        print(f"     â€¢ Overall score: {health['overall_score']:.1f}/100")
        print(f"     â€¢ Issues found: {len(health['issues'])}")

        print("âœ… Simulation system working correctly")
        return True

    except Exception as e:
        print(f"âŒ Simulation test failed: {e}")
        return False


def test_file_structure():
    """Verificar estructura de archivos"""

    print("\nğŸ“ Testing File Structure...")

    required_files = [
        "scripts/auto_update_docs.py",
        "scripts/documentation_dashboard.py",
        "scripts/generate_simulation_data.py",
        "scripts/setup_documentation_system.sh",
        "docs/functionality_guides/README_SYSTEM.md",
    ]

    missing_files = []

    for file_path in required_files:
        full_path = repo_root / file_path
        if full_path.exists():
            size_kb = full_path.stat().st_size / 1024
            print(f"   âœ… {file_path} ({size_kb:.1f} KB)")
        else:
            print(f"   âŒ {file_path} (missing)")
            missing_files.append(file_path)

    if missing_files:
        print(f"âŒ Missing {len(missing_files)} required files")
        return False
    else:
        print("âœ… All required files present")
        return True


def test_imports():
    """Probar importaciones y dependencias"""

    print("\nğŸ Testing Python Imports...")

    # Core imports que siempre deben funcionar
    core_modules = [
        "os",
        "sys",
        "json",
        "datetime",
        "pathlib",
        "typing",
        "collections",
        "dataclasses",
        "asyncio",
        "re",
        "ast",
    ]

    for module in core_modules:
        try:
            __import__(module)
            print(f"   âœ… {module}")
        except ImportError as e:
            print(f"   âŒ {module}: {e}")
            return False

    # Optional imports
    optional_modules = [
        ("streamlit", "Web dashboard functionality"),
        ("plotly", "Interactive charts"),
        ("pandas", "Data analysis"),
        ("git", "Git integration"),
    ]

    print("\n   Optional dependencies:")
    for module, description in optional_modules:
        try:
            __import__(module)
            print(f"   âœ… {module} - {description}")
        except ImportError:
            print(f"   âš ï¸  {module} - {description} (fallback mode available)")

    print("âœ… Import testing completed")
    return True


def run_interactive_demo():
    """Ejecutar demo interactivo"""

    print("\nğŸ® Interactive Demo Mode")
    print("=" * 30)

    try:
        from scripts.documentation_dashboard import CLIDashboard

        # Crear CLI dashboard
        cli = CLIDashboard()

        print("Starting CLI Dashboard...")
        cli.run_cli()

    except KeyboardInterrupt:
        print("\nğŸ‘‹ Demo interrupted by user")
    except Exception as e:
        print(f"\nâŒ Demo failed: {e}")


def main():
    """FunciÃ³n principal de testing"""

    print("ğŸ§ª Documentation System Test Suite")
    print("=" * 50)

    tests = [
        ("File Structure", test_file_structure),
        ("Python Imports", test_imports),
        ("Auto-Update System", test_auto_updater),
        ("Dashboard System", test_dashboard_cli),
        ("Simulation Data", test_simulation_data),
    ]

    results = []

    for test_name, test_func in tests:
        print(f"\nğŸ§ª Running: {test_name}")
        try:
            success = test_func()
            results.append((test_name, success))
        except Exception as e:
            print(f"âŒ {test_name} crashed: {e}")
            results.append((test_name, False))

    # Resumen final
    print("\n" + "=" * 50)
    print("ğŸ“‹ Test Results Summary")
    print("=" * 50)

    passed = 0
    for test_name, success in results:
        status = "âœ… PASS" if success else "âŒ FAIL"
        print(f"{status:10} {test_name}")
        if success:
            passed += 1

    print(f"\nğŸ“Š Results: {passed}/{len(results)} tests passed")

    if passed == len(results):
        print("ğŸ‰ All tests passed! System is ready to use.")

        # Ofrecer demo interactivo
        response = input("\nğŸ® Run interactive demo? (y/N): ").strip().lower()
        if response == "y":
            run_interactive_demo()
    else:
        print("âš ï¸  Some tests failed. Check the output above for details.")

    return 0 if passed == len(results) else 1


if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Testing interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\nğŸ’¥ Unexpected error: {e}")
        sys.exit(1)
